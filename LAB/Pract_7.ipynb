{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1zWZJoHyXjKe41gE-tsLay3XZ89IhlsfX","timestamp":1714526284165}],"authorship_tag":"ABX9TyN7sJFart9HxEFMxnR4/DgF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c54SI6NnSJ1b","executionInfo":{"status":"ok","timestamp":1714554202807,"user_tz":-330,"elapsed":1735,"user":{"displayName":"Sahil Deshmukh","userId":"03530822948290175803"}},"outputId":"22bb4e6c-b1a2-401c-e3bc-18996d16f5e7"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":24}],"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')"]},{"cell_type":"code","source":["text= \"Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.\""],"metadata":{"id":"J12vpPdvVvi5","executionInfo":{"status":"ok","timestamp":1714554204520,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sahil Deshmukh","userId":"03530822948290175803"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["#Sentence Tokenization\n","from nltk.tokenize import sent_tokenize\n","tokenized_text= sent_tokenize(text)\n","print(tokenized_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mqbkgIELV3K6","executionInfo":{"status":"ok","timestamp":1714554204520,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sahil Deshmukh","userId":"03530822948290175803"}},"outputId":"2d1b0629-62c5-4a94-b806-23c8ba57fa71"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["['Tokenization is the first step in text analytics.', 'The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.']\n"]}]},{"cell_type":"code","source":["#Word Tokenization\n","from nltk.tokenize import word_tokenize\n","tokenized_word=word_tokenize(text)\n","print(tokenized_word)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z4XLM2d8V7Tv","executionInfo":{"status":"ok","timestamp":1714554204520,"user_tz":-330,"elapsed":7,"user":{"displayName":"Sahil Deshmukh","userId":"03530822948290175803"}},"outputId":"fd72438f-39b1-4c36-cbbb-38603227da45"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["['Tokenization', 'is', 'the', 'first', 'step', 'in', 'text', 'analytics', '.', 'The', 'process', 'of', 'breaking', 'down', 'a', 'text', 'paragraph', 'into', 'smaller', 'chunks', 'such', 'as', 'words', 'or', 'sentences', 'is', 'called', 'Tokenization', '.']\n"]}]},{"cell_type":"code","source":["# print stop words of English\n","from nltk.corpus import stopwords\n","stop_words=set(stopwords.words(\"english\"))\n","print(stop_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YPlaSNJ_WCvm","executionInfo":{"status":"ok","timestamp":1714554204521,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sahil Deshmukh","userId":"03530822948290175803"}},"outputId":"1c9f9423-4525-4327-d26c-27b8554cde21"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["{'up', 'has', 'will', 'd', 'an', 'being', 'haven', \"couldn't\", 'your', 'himself', 'the', 'each', 'same', 'ours', 'does', 'such', 'needn', 'until', 'why', 'most', 'she', 'down', 'been', 'between', 'ain', 'having', 'this', 'weren', 'then', \"mightn't\", 'mustn', \"you're\", 'myself', \"you'll\", 'their', 'o', 'here', \"won't\", 'of', \"wasn't\", 'itself', 'them', 'wouldn', 'under', \"you've\", \"that'll\", 'from', 'mightn', \"mustn't\", 'doing', \"hadn't\", 'in', 'is', 'over', 'll', 'he', 'and', 'but', 'that', 'themselves', 'they', 'am', 'should', 'me', 'with', 'off', 'own', 'couldn', 'don', 'before', 'shan', 'than', 's', \"don't\", \"should've\", 't', 'those', 'once', 'you', 'again', 'only', 'i', \"weren't\", 'at', 'not', 'didn', 'hadn', \"you'd\", 'be', 'where', 'y', 'hers', 'because', 've', 'can', 'which', 'had', 'above', 'what', 'it', 'below', \"she's\", 'did', 'there', 'by', 'whom', 'as', 'theirs', 'both', \"shan't\", 'few', 'out', 'while', 'into', 'ma', 'we', 'our', 'his', 'her', 'now', 'yourself', 'just', \"doesn't\", \"needn't\", \"hasn't\", 'a', 'when', 'more', 'wasn', 'on', \"it's\", 'm', 'nor', 'yourselves', 'who', 'very', 'no', 'do', 'have', 'if', 'these', \"shouldn't\", 'so', 'aren', 'or', 'my', 'yours', 'doesn', 'its', 'how', \"haven't\", 'to', \"didn't\", 'further', 'were', 'him', 'after', 'was', 'herself', 'during', 'shouldn', 'any', 'isn', 'won', \"aren't\", 'about', 'ourselves', 'some', \"wouldn't\", 'hasn', 'against', 'other', 'all', 'through', 'are', 'too', 're', 'for', \"isn't\"}\n"]}]},{"cell_type":"code","source":["import re\n","text= \"How to remove stop words with NLTK library in Python?\"\n","text= re.sub('[^a-zA-Z]', ' ',text)\n","tokens = word_tokenize(text.lower())\n","filtered_text=[]\n","for w in tokens:\n","  if w not in stop_words:\n","    filtered_text.append(w)\n","print(\"Tokenized Sentence:\",tokens)\n","print(\"Filterd Sentence:\",filtered_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T2Xi02W7WIcT","executionInfo":{"status":"ok","timestamp":1714554204521,"user_tz":-330,"elapsed":7,"user":{"displayName":"Sahil Deshmukh","userId":"03530822948290175803"}},"outputId":"e6fda77d-dac0-44d7-9388-257106c2cedb"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized Sentence: ['how', 'to', 'remove', 'stop', 'words', 'with', 'nltk', 'library', 'in', 'python']\n","Filterd Sentence: ['remove', 'stop', 'words', 'nltk', 'library', 'python']\n"]}]},{"cell_type":"markdown","source":["from nltk.stem import PorterStemmer\n","e_words= [\"wait\", \"waiting\", \"waited\", \"waits\"]\n","ps =PorterStemmer()\n","for w in e_words:\n","  rootWord=ps.stem(w)\n","print(rootWord)"],"metadata":{"id":"Gma_wL2RWpRD"}},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer\n","wordnet_lemmatizer = WordNetLemmatizer()\n","text = \"studies studying cries cry\"\n","tokenization = nltk.word_tokenize(text)\n","for w in tokenization:\n","  print(\"Lemma for {} is {}\".format(w,\n","  wordnet_lemmatizer.lemmatize(w)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"db4hUiafWuZ3","executionInfo":{"status":"ok","timestamp":1714554204521,"user_tz":-330,"elapsed":6,"user":{"displayName":"Sahil Deshmukh","userId":"03530822948290175803"}},"outputId":"470bc544-9145-4f4a-c595-35ce457daa59"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Lemma for studies is study\n","Lemma for studying is studying\n","Lemma for cries is cry\n","Lemma for cry is cry\n"]}]},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","data=\"The pink sweater fit her perfectly\"\n","words=word_tokenize(data)\n","for word in words:\n","  print(nltk.pos_tag([word]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5xiTlnnGW3r6","executionInfo":{"status":"ok","timestamp":1714554204521,"user_tz":-330,"elapsed":5,"user":{"displayName":"Sahil Deshmukh","userId":"03530822948290175803"}},"outputId":"4d5b94c4-8af4-4c02-c17a-286a2841fbcd"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["[('The', 'DT')]\n","[('pink', 'NN')]\n","[('sweater', 'NN')]\n","[('fit', 'NN')]\n","[('her', 'PRP$')]\n","[('perfectly', 'RB')]\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer"],"metadata":{"id":"kxcnfbPpW8Ho","executionInfo":{"status":"ok","timestamp":1714554204521,"user_tz":-330,"elapsed":5,"user":{"displayName":"Sahil Deshmukh","userId":"03530822948290175803"}}},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":["documentA = 'Jupiter is the largest Planet'\n","documentB = 'Mars is the fourth planet from the Sun'"],"metadata":{"id":"ldA2s9DKW9v_"}},{"cell_type":"code","source":["documentA = 'Jupiter is the largest Planet'\n","documentB = 'Mars is the fourth planet from the Sun'\n","bagOfWordsA = documentA.split(' ')\n","bagOfWordsB = documentB.split(' ')"],"metadata":{"id":"ty7PKkHlXFRt","executionInfo":{"status":"ok","timestamp":1714554204521,"user_tz":-330,"elapsed":4,"user":{"displayName":"Sahil Deshmukh","userId":"03530822948290175803"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))"],"metadata":{"id":"0YsdeBgYXIyF","executionInfo":{"status":"ok","timestamp":1714554204521,"user_tz":-330,"elapsed":4,"user":{"displayName":"Sahil Deshmukh","userId":"03530822948290175803"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["numOfWordsA = dict.fromkeys(uniqueWords, 0)\n","for word in bagOfWordsA:\n","  numOfWordsA[word] += 1\n","  numOfWordsB = dict.fromkeys(uniqueWords, 0)\n","for word in bagOfWordsB:\n","  numOfWordsB[word] += 1"],"metadata":{"id":"r2lDH4fpXLPN","executionInfo":{"status":"ok","timestamp":1714554204521,"user_tz":-330,"elapsed":4,"user":{"displayName":"Sahil Deshmukh","userId":"03530822948290175803"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["def computeTF(wordDict, bagOfWords):\n","  tfDict = {}\n","  bagOfWordsCount = len(bagOfWords)\n","  for word, count in wordDict.items():\n","    tfDict[word] = count / float(bagOfWordsCount)\n","  return tfDict\n","  tfA = computeTF(numOfWordsA, bagOfWordsA)\n","  tfB = computeTF(numOfWordsB, bagOfWordsB)"],"metadata":{"id":"dj1yhOLTXOHk","executionInfo":{"status":"ok","timestamp":1714554204522,"user_tz":-330,"elapsed":5,"user":{"displayName":"Sahil Deshmukh","userId":"03530822948290175803"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["def computeIDF(documents):\n","  import math\n","  N = len(documents)\n","  idfDict = dict.fromkeys(documents[0].keys(), 0)\n","  for document in documents:\n","    for word, val in document.items():\n","      if val > 0:\n","        idfDict[word] += 1\n","  for word, val in idfDict.items():\n","    idfDict[word] = math.log(N / float(val))\n","  return idfDict\n","  idfs = computeIDF([numOfWordsA, numOfWordsB])\n"],"metadata":{"id":"6DaRcfcYXkry","executionInfo":{"status":"ok","timestamp":1714554204522,"user_tz":-330,"elapsed":5,"user":{"displayName":"Sahil Deshmukh","userId":"03530822948290175803"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["def computeTFIDF(tfBagOfWords, idfs):\n","  tfidf = {}\n","  for word, val in tfBagOfWords.items():\n","    tfidf[word] = val * idfs[word]\n","  return tfidf\n","  tfidfA = computeTFIDF(tfA, idfs)\n","  tfidfB = computeTFIDF(tfB, idfs)\n","  df = pd.DataFrame([tfidfA, tfidfB])\n","  df"],"metadata":{"id":"NQgHDUZKXRYT","executionInfo":{"status":"ok","timestamp":1714554204522,"user_tz":-330,"elapsed":4,"user":{"displayName":"Sahil Deshmukh","userId":"03530822948290175803"}}},"execution_count":38,"outputs":[]}]}